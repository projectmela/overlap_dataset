{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b00999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30c7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Annotation set\n",
    "anno_set = 'Set 3/'\n",
    "\n",
    "## Names of files where mapping is being performed\n",
    "file1 = '20230313_SE_Lek1_P1D2_DJI_0020'\n",
    "file2 = '20230313_SE_Lek1_P2D4_DJI_0940'\n",
    "\n",
    "## Names of files compressed\n",
    "f1 = 'P1D2_DJI_0020'\n",
    "f2 = 'P2D4_DJI_0940'\n",
    "\n",
    "## Column names used often\n",
    "start_frame1 = 'start_frame_' + f1\n",
    "start_frame2 = 'start_frame_' + f2\n",
    "end_frame1 = 'end_frame_' + f1\n",
    "end_frame2 = 'end_frame_' + f2\n",
    "lagged_start_frame1 = 'lagged_start_frame_' + f1\n",
    "lagged_start_frame2 = 'lagged_start_frame_' + f2\n",
    "lagged_end_frame1 = 'lagged_end_frame_' + f1\n",
    "lagged_end_frame2 = 'lagged_end_frame_' + f2\n",
    "Frame1 = 'Frame_' + f1\n",
    "Frame2 = 'Frame_' + f2\n",
    "ID1 = 'ID_' + f1\n",
    "ID2 = 'ID_' + f2\n",
    "\n",
    "## Boolean to decide if initial frame labelled videos are to be created\n",
    "preliminary_videos = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3feafe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read dataframes for both files\n",
    "df1 = pd.read_csv('/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/data/' + file1 + '_tracks.csv')\n",
    "df1 = df1.loc[:,['frame', 'id', 'bb_left', 'bb_top', 'bb_width', 'bb_height']]\n",
    "df2 = pd.read_csv('/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/data/' + file2 + '_tracks.csv')\n",
    "df2 = df2.loc[:,['frame', 'id', 'bb_left', 'bb_top', 'bb_width', 'bb_height']]\n",
    "\n",
    "## Read the manually created mapping file\n",
    "map_df = pd.read_csv('/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Manuscripts/10_dataset/Data/' + anno_set + 'Mapping_' + file1 + '_' + file2 + '.csv', sep=';')\n",
    "map_df.columns = [Frame1, ID1, Frame2, ID2]\n",
    "map_df = map_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "## Frame mapping for overlap (vid2 - vid1) e.g. lag = 3 implies vid1(396) == vid2(399)\n",
    "lag = 594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b51414",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incorporate lag in the tracking data\n",
    "if lag > 0:\n",
    "    df1['lagged_frame'] = df1['frame'] + np.abs(lag)\n",
    "    df2['lagged_frame'] = df2['frame']\n",
    "elif lag < 0:\n",
    "    df1['lagged_frame'] = df1['frame']\n",
    "    df2['lagged_frame'] = df2['frame'] + np.abs(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48e2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to find min and max in an array within the 'current chain'\n",
    "def find_chain_min_max(array, element):\n",
    "    ## Find the index of the given element in the array\n",
    "    index = np.where(array == element)[0][0]\n",
    "    \n",
    "    ## Find the starting index of the consecutive chain\n",
    "    start_index = index\n",
    "    while start_index > 0 and array[start_index] - array[start_index - 1] == 1:\n",
    "        start_index -= 1\n",
    "    \n",
    "    ## Find the ending index of the consecutive chain\n",
    "    end_index = index\n",
    "    while end_index < len(array) - 1 and array[end_index + 1] - array[end_index] == 1:\n",
    "        end_index += 1\n",
    "    \n",
    "    ## Return the minimum and maximum within the consecutive chain\n",
    "    return array[start_index], array[end_index]\n",
    "\n",
    "## Function to calculate overlap frames\n",
    "def calculate_overlap(row):\n",
    "    start_overlap = max(row[lagged_start_frame1], row[lagged_start_frame2])\n",
    "    end_overlap = min(row[lagged_end_frame1], row[lagged_end_frame2])\n",
    "    if start_overlap <= end_overlap:\n",
    "        return pd.Series([start_overlap, end_overlap])\n",
    "    else:\n",
    "        return pd.Series([None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af8547",
   "metadata": {},
   "source": [
    "### Write the two videos with frame numbers printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e5b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get paths and parameters for creating the output videos\n",
    "input_vidpath1 = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/data/' + file1 + '.MP4'\n",
    "input_vidpath2 = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/data/' + file2 + '.MP4'\n",
    "output_vidpath1 = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/output/tracks/' + anno_set + file1 + '_frameno.mp4'\n",
    "output_vidpath2 = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/output/tracks/' + anno_set + file2 + '_frameno.mp4'\n",
    "codec = 'DIVX'\n",
    "scaling = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f4d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preliminary_videos:    \n",
    "    ## Write output video 1\n",
    "    ## Open video\n",
    "    cap = cv2.VideoCapture(input_vidpath1)\n",
    "    if cap.isOpened() == False:\n",
    "        sys.exit('Video file cannot be read! Please check input_vidpath to ensure it is correctly pointing to the video file')\n",
    "\n",
    "    ## Video writer class to output video with contour and centroid of tracked object(s)\n",
    "    # make sure the frame size matches size of array 'final'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    output_framesize = (int(cap.read()[1].shape[1]*scaling),int(cap.read()[1].shape[0]*scaling))\n",
    "    out = cv2.VideoWriter(filename = output_vidpath1, fourcc = fourcc, fps = 30.0, frameSize = output_framesize, isColor = True)\n",
    "\n",
    "    last = 0\n",
    "\n",
    "    while True:\n",
    "        ## Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        this = cap.get(1)\n",
    "\n",
    "        if ret == True:\n",
    "            ## Preprocess the image for background subtraction\n",
    "            frame = cv2.resize(frame, None, fx=scaling, fy=scaling, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            cv2.putText(frame, str(this), (20,50), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "\n",
    "            ## Display the resulting frame\n",
    "            out.write(frame)\n",
    "            cv2.imshow('frame1', frame)\n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "\n",
    "        if last >= this:\n",
    "            break\n",
    "\n",
    "        last = this\n",
    "\n",
    "    ## When everything done, release the capture\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69709dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preliminary_videos:\n",
    "    ## Write output video 2\n",
    "    ## Open video\n",
    "    cap = cv2.VideoCapture(input_vidpath2)\n",
    "    if cap.isOpened() == False:\n",
    "        sys.exit('Video file cannot be read! Please check input_vidpath to ensure it is correctly pointing to the video file')\n",
    "\n",
    "    ## Video writer class to output video with contour and centroid of tracked object(s)\n",
    "    # make sure the frame size matches size of array 'final'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    output_framesize = (int(cap.read()[1].shape[1]*scaling),int(cap.read()[1].shape[0]*scaling))\n",
    "    out = cv2.VideoWriter(filename = output_vidpath2, fourcc = fourcc, fps = 30.0, frameSize = output_framesize, isColor = True)\n",
    "\n",
    "    last = 0\n",
    "\n",
    "    while True:\n",
    "        ## Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        this = cap.get(1)\n",
    "\n",
    "        if ret == True:\n",
    "            ## Preprocess the image for background subtraction\n",
    "            frame = cv2.resize(frame, None, fx=scaling, fy=scaling, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            cv2.putText(frame, str(this), (20,50), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "\n",
    "            ## Display the resulting frame\n",
    "            out.write(frame)\n",
    "            cv2.imshow('frame2', frame)\n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "\n",
    "        if last >= this:\n",
    "            break\n",
    "\n",
    "        last = this\n",
    "\n",
    "    ## When everything done, release the capture\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb850c77",
   "metadata": {},
   "source": [
    "### Make a dataframe that has the same rows as  'map_df' but includes start and end frames of each ID in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e251efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ready new mapping dataframe with start and end frames for each ID\n",
    "for index, row in map_df.iterrows():\n",
    "    ## Get tracklets from current row\n",
    "    tracklet1 = df1[df1['id'] == row[ID1]]\n",
    "    tracklet2 = df2[df2['id'] == row[ID2]]\n",
    "    \n",
    "    ## Get start and end frame of tracklets based on annotated frame\n",
    "    anno_fr1 = row[Frame1]\n",
    "    anno_fr2 = row[Frame2]\n",
    "    \n",
    "    start_fr1, end_fr1 = find_chain_min_max(np.array(tracklet1['frame']), anno_fr1)\n",
    "    start_fr2, end_fr2 = find_chain_min_max(np.array(tracklet2['frame']), anno_fr2)\n",
    "    \n",
    "    end_fr1 = np.max(df1['frame']) if end_fr1 > np.max(df1['frame']) else end_fr1\n",
    "    end_fr2 = np.max(df2['frame']) if end_fr2 > np.max(df2['frame']) else end_fr2\n",
    "    \n",
    "    lagged_start_fr1 = tracklet1.loc[tracklet1['frame'] == start_fr1, 'lagged_frame'].values[0]\n",
    "    lagged_start_fr2 = tracklet2.loc[tracklet2['frame'] == start_fr2, 'lagged_frame'].values[0]\n",
    "    lagged_end_fr1 = tracklet1.loc[tracklet1['frame'] == end_fr1, 'lagged_frame'].values[0]\n",
    "    lagged_end_fr2 = tracklet2.loc[tracklet2['frame'] == end_fr2, 'lagged_frame'].values[0]\n",
    "    \n",
    "    ## Compile modified dataframe\n",
    "    tmp = pd.DataFrame([start_fr1, end_fr1, lagged_start_fr1, lagged_end_fr1, row[ID1], \n",
    "                        start_fr2, end_fr2, lagged_start_fr2, lagged_end_fr2, row[ID2]]).T\n",
    "    tmp.columns = [start_frame1, end_frame1, lagged_start_frame1, lagged_end_frame1, ID1, \n",
    "                   start_frame2, end_frame2, lagged_start_frame2, lagged_end_frame2, ID2]\n",
    "    if index != 0:\n",
    "        df = pd.concat((df, tmp), axis=0)\n",
    "    else:\n",
    "        df = tmp\n",
    "        \n",
    "df = df.reset_index(drop = True)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eda8f",
   "metadata": {},
   "source": [
    "Errors detected were manually deleted in the mapping file. These rows were deleted because the frame indicated in the map_df dataframe was outside of the life of the corresponding ID.\n",
    "\n",
    "I also manually added lines at the end of the mapping files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b53093",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate overlap between the two videos\n",
    "overlap_frames = df.apply(calculate_overlap, axis=1)\n",
    "overlap_frames.columns = ['start_frame_overlap', 'end_frame_overlap']\n",
    "\n",
    "## Add the calculated overlap frames to the original DataFrame\n",
    "df = pd.concat([df, overlap_frames], axis=1)\n",
    "df['overlap_duration'] = df['end_frame_overlap'] - df['start_frame_overlap'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc54ece",
   "metadata": {},
   "source": [
    "### Plot videos of the two videos with colours to indicate corresponding tracklets in the two videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6b8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get paths and parameters for creating the output videos\n",
    "output_vidpath1 = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/output/tracks/' + anno_set + file1 + '_tracked.mp4'\n",
    "output_vidpath2 = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/output/tracks/' + anno_set + file2 + '_tracked.mp4'\n",
    "output_vidpath_merged = '/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/output/tracks/' + anno_set + file1 + '_' + file2 + '_merged.mp4'\n",
    "\n",
    "## Plotting parameters\n",
    "max_radius = 5\n",
    "col_mult = int(255/len(df))\n",
    "colours = [(i*col_mult,255,255) for i in range(len(df))]\n",
    "\n",
    "## Include the colour column to df\n",
    "df['colour'] = colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73f19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate positions of all individuals in both videos\n",
    "df1['pos_x'] = (df1['bb_left'] + df1['bb_width']/2)*scaling\n",
    "df1['pos_y'] = (df1['bb_top'] + df1['bb_height']/2)*scaling\n",
    "df2['pos_x'] = (df2['bb_left'] + df2['bb_width']/2)*scaling\n",
    "df2['pos_y'] = (df2['bb_top'] + df2['bb_height']/2)*scaling\n",
    "\n",
    "## Number of frames to plot\n",
    "fr_min = int(np.min([np.min(df1['lagged_frame']), np.min(df2['lagged_frame'])]))\n",
    "fr_max = int(np.max([np.max(df1['lagged_frame']), np.max(df2['lagged_frame'])]))\n",
    "N = fr_max - fr_min + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f10b381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write video 1\n",
    "## Read input video\n",
    "cap = cv2.VideoCapture(input_vidpath1)\n",
    "if cap.isOpened() == False:\n",
    "    sys.exit('Video file cannot be read! Please check input_vidpath to ensure it is correctly pointing to the video file')\n",
    "\n",
    "## Get properties of the input video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)*scaling)\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)*scaling)\n",
    "\n",
    "## Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*codec)  # Specify codec\n",
    "out = cv2.VideoWriter(output_vidpath1, fourcc, fps, (width, height))\n",
    "\n",
    "## Prepare a black frame\n",
    "black_frame = np.zeros((height, width, 3), np.uint8)\n",
    "\n",
    "## Create a dictionary for quick lookup of frames to be copied\n",
    "if lag > 0:\n",
    "    lagged_frames = set(df1['lagged_frame'].tolist())\n",
    "elif lag < 0:\n",
    "    lagged_frames = set(df2['lagged_frame'].tolist())\n",
    "\n",
    "## Initialize a counter for reading frames from input video\n",
    "input_frame_idx = 0\n",
    "\n",
    "for i in range(N):\n",
    "    if i in lagged_frames:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, None, fx=scaling, fy=scaling, interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            ## Filter objects of interest based on the current frame\n",
    "            filtered_df = df[(df['start_frame_overlap'] <= i) & (df['end_frame_overlap'] >= i)]\n",
    "            if filtered_df.empty == False:\n",
    "                indices, interested_objects = zip(*[(idx, row[ID1]) for idx, row in filtered_df.iterrows()])\n",
    "\n",
    "                ## Filter MOT dataframe based on the current frame and interested objects\n",
    "                tmp = df1[(df1['lagged_frame'] == i) & (df1['id'].isin(interested_objects))]\n",
    "                tmp = tmp.reset_index()\n",
    "\n",
    "                for j in range(0,len(tmp)):\n",
    "                    x = int(tmp.loc[j,'pos_x'])\n",
    "                    y = int(tmp.loc[j,'pos_y'])\n",
    "                    r = 7\n",
    "                    c = df.loc[indices[np.where(interested_objects == tmp.loc[j,'id'])[0][0]], 'colour']\n",
    "                    cv2.circle(frame, (x,y), r, c, -1, cv2.LINE_AA)\n",
    "                    cv2.putText(frame, str(int(tmp.loc[j,'id'])), (x+20,y-20), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.putText(frame, str(input_frame_idx), (20,50), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 255), 2)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            out.write(frame)\n",
    "            cv2.imshow('output1', frame)  ## Display the frame\n",
    "            input_frame_idx += 1\n",
    "        else:\n",
    "            ## In case there's an issue reading a frame, write a black frame\n",
    "            out.write(black_frame)\n",
    "            cv2.imshow('output1', black_frame)  ## Display the black frame\n",
    "    else:\n",
    "        out.write(black_frame)\n",
    "        cv2.imshow('output1', black_frame)  ## Display the black frame\n",
    "        \n",
    "    ## Exit if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "## Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff3dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "## Write video 2\n",
    "## Read input video\n",
    "cap = cv2.VideoCapture(input_vidpath2)\n",
    "if cap.isOpened() == False:\n",
    "    sys.exit('Video file cannot be read! Please check input_vidpath to ensure it is correctly pointing to the video file')\n",
    "\n",
    "## Get properties of the input video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)*scaling)\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)*scaling)\n",
    "\n",
    "## Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*codec)  # Specify codec\n",
    "out = cv2.VideoWriter(output_vidpath2, fourcc, fps, (width, height))\n",
    "\n",
    "## Prepare a black frame\n",
    "black_frame = np.zeros((height, width, 3), np.uint8)\n",
    "\n",
    "## Create a dictionary for quick lookup of frames to be copied\n",
    "if lag > 0:\n",
    "    lagged_frames = set(df2['lagged_frame'].tolist())\n",
    "elif lag < 0:\n",
    "    lagged_frames = set(df1['lagged_frame'].tolist())\n",
    "\n",
    "## Initialize a counter for reading frames from input video\n",
    "input_frame_idx = 0\n",
    "\n",
    "for i in range(N):\n",
    "    if i in lagged_frames:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, None, fx=scaling, fy=scaling, interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            ## Filter objects of interest based on the current frame\n",
    "            filtered_df = df[(df['start_frame_overlap'] <= i) & (df['end_frame_overlap'] >= i)]\n",
    "            if filtered_df.empty == False:\n",
    "                indices, interested_objects = zip(*[(idx, row[ID2]) for idx, row in filtered_df.iterrows()])\n",
    "\n",
    "                ## Filter MOT dataframe based on the current frame and interested objects\n",
    "                tmp = df2[(df2['lagged_frame'] == i) & (df2['id'].isin(interested_objects))]\n",
    "                tmp = tmp.reset_index()\n",
    "\n",
    "                for j in range(0,len(tmp)):\n",
    "                    x = int(tmp.loc[j,'pos_x'])\n",
    "                    y = int(tmp.loc[j,'pos_y'])\n",
    "                    r = 7\n",
    "                    c = df.loc[indices[np.where(interested_objects == tmp.loc[j,'id'])[0][0]], 'colour']\n",
    "                    cv2.circle(frame, (x,y), r, c, -1, cv2.LINE_AA)\n",
    "                    cv2.putText(frame, str(int(tmp.loc[j,'id'])), (x+20,y-20), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.putText(frame, str(input_frame_idx), (20,50), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 255), 2)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            out.write(frame)\n",
    "            cv2.imshow('output2', frame)  ## Display the frame\n",
    "            input_frame_idx += 1\n",
    "        else:\n",
    "            ## In case there's an issue reading a frame, write a black frame\n",
    "            out.write(black_frame)\n",
    "            cv2.imshow('output2', black_frame)  ## Display the black frame\n",
    "    else:\n",
    "        out.write(black_frame)\n",
    "        cv2.imshow('output2', black_frame)  ## Display the black frame\n",
    "        \n",
    "    ## Exit if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "## Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write the concatenated video\n",
    "## Read both input videos\n",
    "cap1 = cv2.VideoCapture(output_vidpath1)\n",
    "if not cap1.isOpened():\n",
    "    sys.exit('Video file 1 cannot be read! Please check input_vidpath1 to ensure it is correctly pointing to the video file')\n",
    "\n",
    "cap2 = cv2.VideoCapture(output_vidpath2)\n",
    "if not cap2.isOpened():\n",
    "    sys.exit('Video file 2 cannot be read! Please check input_vidpath2 to ensure it is correctly pointing to the video file')\n",
    "\n",
    "## Video writer class to output concatenated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "output_framesize = (int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH) * scaling), int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT) * scaling * 2))\n",
    "out = cv2.VideoWriter(filename=output_vidpath_merged, fourcc=fourcc, fps=30.0, frameSize=output_framesize, isColor=True)\n",
    "\n",
    "## Loop through frames of both videos simultaneously and concatenate vertically\n",
    "while(True):\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "\n",
    "    ## Check if either video has reached the end\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "    ## Resize frames\n",
    "    frame1 = cv2.resize(frame1, None, fx=scaling, fy=scaling, interpolation=cv2.INTER_LINEAR)\n",
    "    frame2 = cv2.resize(frame2, None, fx=scaling, fy=scaling, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    ## Concatenate frames vertically\n",
    "    concatenated_frame = cv2.vconcat([frame1, frame2])\n",
    "\n",
    "    ## Write the concatenated frame to the output video\n",
    "    out.write(concatenated_frame)\n",
    "\n",
    "    ## Display the concatenated frame\n",
    "    cv2.imshow('output_merged', concatenated_frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "## When everything done, release the capture\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250779ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save modified dataframe\n",
    "df = df.drop(['start_frame_overlap', 'end_frame_overlap'], axis=1)\n",
    "df.to_csv('/Users/vivekhsridhar/Library/Mobile Documents/com~apple~CloudDocs/Documents/Code/Python/OpenCV/plot_tracks/output/tracks/' + anno_set + file1 + '_' + file2 + '.csv', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
